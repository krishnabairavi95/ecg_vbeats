{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skimage\n",
    "from skimage.io import imread_collection\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage import data, img_as_float\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import cv2\n",
    "from copy import deepcopy\n",
    "import glob\n",
    "import cv2 \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Input Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathway = \"/Users/krishnabairavi/Downloads/train/*.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [mpimg.imread(file) for file in glob.glob(pathway)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 7522, 4)\n"
     ]
    }
   ],
   "source": [
    "print (images[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure followed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. On visual inspection, it can be observed that for every image graph, there are 10 rows and 100 column gridboxes. On visualising the size of the image, the  height and width correspond to approximately  750 X 7500. Thus each gridbox unit can be identified as a box of dimension 75x75. Now, for every image in trainset, I iterate through the gridboxes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The annotated labels present in blue ( in case of non- V) and green color ( in case of V) needs to be indentified as a next step to generate a train set that contains the amplitude of the corresponding peak for both  V and Non-V's. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. This amplitude for each peak in a signal is obtained by detecting the black colored signals and using Harris point detectors to find the peak points  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The generated amplitutdes and the annotations ( obtained from the color of annotations in figures) are used to creat train dataset where the input is amplitude for every signal peak in an image and output is the classified label ( V or Non-V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Logistic Regression classifier is used for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_set = []\n",
    "\n",
    "for img_number in range(len(images)):\n",
    "    \n",
    "    img = cv2.cvtColor(images[img_number], cv2.COLOR_BGRA2BGR)\n",
    "    total_count = 0\n",
    "    \n",
    "  \n",
    "    for i in range(10,7500,75):\n",
    "        \n",
    "        pic = img[0:75,i:i+75]\n",
    "        flag = 0\n",
    "        break_flag = 0\n",
    "        pic_mod = deepcopy(pic)\n",
    "        pic_mod = pic_mod/np.linalg.norm(pic_mod)\n",
    "        sum_pic = np.sum(pic_mod[:,:,2])\n",
    "   \n",
    "        ## Code block to detect blue regions in the annotations ( the one's which are non V's)\n",
    "        \n",
    "        if(sum_pic > 43.5 and sum_pic < 45):\n",
    "            box_number = 0\n",
    "            for m in range(75,750,75):\n",
    "                pic_m = img[m:m+75,i:i+75]\n",
    "            \n",
    "                pic_mod_b = cv2.cvtColor(pic_m, cv2.COLOR_BGR2GRAY)\n",
    "                sum_pic_b = np.sum(pic_mod_b)\n",
    "                \n",
    "                box_number += 1\n",
    "                \n",
    "                ## To find the black peak in an image using color identification and Harris peak detections\n",
    "                \n",
    "                if(sum_pic_b < 5450):\n",
    "                    \n",
    "                    total_count += 1  \n",
    "                    \n",
    "                    operatedImage = np.float32(pic_mod_b)\n",
    "            \n",
    "                    dst = cv2.cornerHarris(operatedImage, 2, 5, 0.01)\n",
    "                    \n",
    "                    dst_new = cv2.dilate(dst,None)\n",
    "\n",
    "                    points=np.unravel_index(dst_new.argmax(),dst.shape)\n",
    "\n",
    "                    append_val_blue = (((4 - box_number)*75) +points[0])*0.5 \n",
    "\n",
    "                    ## Making it a  binary classification problem\n",
    "                    \n",
    "                    ## Blue color represents \"non-V\", which should be determined as class 0\n",
    "                    \n",
    "                    if(append_val_blue < 60):\n",
    "                        feature_set.append( (0, abs((((4 - box_number)*75)+points[0])*0.5 )))\n",
    "                    break\n",
    "\n",
    "                    \n",
    "        ## Code block to detect blue regions in the annotations ( the one's which are non V's)\n",
    "    \n",
    "        if(sum_pic > 42 and sum_pic < 43):\n",
    "            box_number_green = 0\n",
    "\n",
    "        ## To find the black peak in an image using color identification and Harris peak detections\n",
    "            \n",
    "            for m_green in range(75,750,75):\n",
    "                pic_m_green = img[m_green:m_green+75,i:i+75]\n",
    "                pic_mod_b_green = cv2.cvtColor(pic_m_green, cv2.COLOR_BGR2GRAY)\n",
    "                sum_pic_b_green = np.sum(pic_mod_b_green)\n",
    "                box_number_green += 1\n",
    "\n",
    "                if(sum_pic_b_green < 5450):                    \n",
    "            \n",
    "                    operatedImage_green = np.float32(pic_mod_b_green)\n",
    "            \n",
    "                    dst_green = cv2.cornerHarris(operatedImage_green, 2, 5, 0.01)\n",
    "                    \n",
    "                    dst_new_green = cv2.dilate(dst_green,None)\n",
    "\n",
    "                    points_new_green = np.unravel_index(dst_new_green.argmax(),dst_green.shape)\n",
    "\n",
    "                    append_val =  abs((((4 - box_number_green)*75) +points_new_green[0])*0.5 )\n",
    "                    \n",
    "                    ## Green color represents \"V\", which should be determined as class 1\n",
    "                    \n",
    "                    if(abs(append_val) > 60):\n",
    "                        feature_set.append((1, abs((((4 - box_number_green)*75) +points_new_green[0])*0.5 )))\n",
    "                    break\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature_set is a list of tuples, where each tuple represents a class and it's signal peak's amplitude "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the dataset into train and val sets. X-list contains set of amplitudes for every peak and  Y list contains the binary class of the signal ( 0 and 1)\n",
    "\n",
    "x_list=[]\n",
    "y_list=[]\n",
    "for i in feature_set:\n",
    "    y_list.append(i[0])\n",
    "    x_list.append(i[1])\n",
    "\n",
    "x_list = np.asarray(x_list)\n",
    "y_list = np.asarray(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = np.reshape(x_list, (-1,1))\n",
    "y_list = np.reshape(y_list, (-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_list,y_list, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Using Logistic Regression model for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnabairavi/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/krishnabairavi/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=1e-06, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel = LogisticRegression(max_iter=500, tol=0.000001)\n",
    "logmodel.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model to file for retreiving it during testing phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'fin.sav'\n",
    "pickle.dump(logmodel, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
